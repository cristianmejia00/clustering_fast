#==================================================
#Created in R; 2016-03-14; Kajikawa-lab; C.Mejia
#Working fine as of 2018-11-26.
#==================================================

# Call necessary libraries
library(plyr)
library(Opener5)
library(data.table)
library(dplyr)
library(stringr)
library(tm)


choose.files()
file.choose()
###########################################################################################
# OPTIONS
###########################################################################################
## Query_id 
## This has de form Qxxx whith the query number from the query control file
dataset_metadata <- list("query_id" = "Q299 2nd", 
                         "fukan_url" = "Not apply. Directly from WOS")


# Open a window to select the directory with the files to merge
#dir_path = "/Users/cristian/Library/CloudStorage/OneDrive-Personal/Documentos/00-Research projects/58 - GMO - Deals and Patents/Patents/00-Data/Update20240505"
dir_path = "/Users/cristian/Library/CloudStorage/OneDrive-Personal/Documentos/imacros/downloads/Q299 kubota and competitors"
paths_to_files = list.files(path = dir_path, full.names= TRUE, pattern = "*.csv", recursive = TRUE)
paths_to_files = paths_to_files[grepl('.csv$', paths_to_files)]

###########################################################################################
## Path to `/inputs`
# Here 'input' refer to the inputs for clustering. 
# From the pov of this very code, this is actually the output folder. Where the files generated by this code will be placed.
bibliometrics_folder <- "/Users/cristian/Library/CloudStorage/OneDrive-Personal/Documentos/03-bibliometrics" # Mac
#bibliometrics_folder <- "C:\\Users\\crist\\OneDrive\\Documentos\\03-bibliometrics" # Windows
dir.create(file.path(bibliometrics_folder, dataset_metadata$query_id), showWarnings = TRUE)


# Read each file and store them in a vector
# fread sometimes fails when reading the header, what to do?
list_of_all_files <- lapply(paths_to_files, function(a_path){
  data1 <- readr::read_csv(a_path, skip = 1)
  return(data1)})

# Verify than the files have the expected number of rows: 500. Except for a few that were the tails.
plot(unlist(sapply(list_of_all_files, nrow))) #The number of rows in each file, mostly 500.

# Create the merged dataset
dataset <- rbind.fill(list_of_all_files)
dataset <- as.data.frame(dataset)
dataset$X_N <- c(1:nrow(dataset))
# #######################################################################
# #######################################################################
# #######################################################################
# Conversion to WOS format

# The length of the IPC to use, either 3 or 4.
IPC_digits <- 4

# ##########################################
# Helper function to obtain cut IPC code
# x is the vector of IPC in text format
# digits is the size of the cut
# unique_ipc = TRUE: Get all unique instances of IPC found in the patent, sorted from the most frequent
#              FALSE: Just let repeated values there.  
IPC_list <-  function(IPC_column, digits = 4, unique_ipc = TRUE) { 
  members <- strsplit(IPC_column, split =" \\| ")
  members <- lapply(members, function(x) x[x!=""])
  members <- lapply(members, unique)
  members <- lapply(members, function(x) substring(x, 1, digits))
  if (unique_ipc) {
    members <- lapply(members, function(x) {
      temp <- table(x) %>% 
        sort(., decreasing = TRUE) %>% 
        names
      return(temp)
    })
  }
  return (lapply(members, function(x) {paste(x, collapse = " | ")}))}

# Change the contents to desired formats
# IPC of 4 digits
dataset$IPC_full <- dataset$`IPC - Current - DWPI` # Back it up
dataset$`IPC - Current - DWPI` <- IPC_list(dataset$`IPC - Current - DWPI`, IPC_digits)

# Change the pipe separator to semicolon as in WOS
piped_columns <- c(
  "IPC_full",
  "IPC - Current - DWPI",
  "Assignee/Applicant", 
  "Assignee - Current US",         
  "Optimized Assignee", 
  "Ultimate Parent",
  "Cited Refs - Patent",
  "DWPI Family Members"
)

# Convert to ; for what is available
available_piped_columns <- piped_columns[piped_columns %in% colnames(dataset)]
for (ii in available_piped_columns){
  dataset[,ii] <- gsub(" \\| ", "; ", dataset[,ii]) %>% as.character()
}

# Change names to equivalents
new_names <- c(
  UT =  "Publication Number",
  TI =  "Title - DWPI",
  AB = "Abstract - DWPI",
  WC = "IPC - Current - DWPI",
  Country = "Publication Country Code",
  SO = "Ultimate Parent", 
  PY = "Publication Year",
  Z9 = "Count of Citing Patents - DPCI",
  AU = "Assignee/Applicant"
)
dataset <- dataset %>% rename(all_of(new_names))


# Prepare keywords
# Remove stopwords, numbers, and symbols from the 'text' column
dataset$DE <- dataset$TI %>%
  tolower() %>%  # Convert text to lowercase
  removeNumbers() %>%  # Remove numbers
  removePunctuation() %>%  # Remove punctuation
  removeWords(stopwords("english")) %>%  # Remove English stopwords
  stripWhitespace() %>% # Remove extra whitespace
  str_replace_all("\\s+", "; ") # Replace whitespace with a semicolon in the 'text_cleaned' column

# custom edits:
# Remove keywords tending to appear in patent text that are irrelevant
dataset$DE <- gsub(' eg;', '', dataset$DE) # from "e.g."


#########################
# Format

# Correct records without PY
table(dataset$PY)
table(is.na(dataset$PY))

# Corrections to Z9
dataset$Z9[is.na(dataset$Z9)] <- 0


# Remove duplicated files
dataset = dataset %>% filter(!duplicated(UT))


#########################
# Custom columns
# This time I dont need Authors, as the Ultimate Parent will be used. 
# I'll change the Authors to the main dataset name

# Check current ultimate parents
dataset$SO %>% 
  strsplit("; ") %>% 
  unlist() %>%
  table() %>%
  sort(decreasing = TRUE) %>%
  .[1:20]

# Add the corresponding dataset' author
dataset$AU <- "OTHER"
dataset$AU[grepl("CNH|BLUE LEAF", dataset$SO)] <- "CNH"
dataset$AU[grepl("DEERE", dataset$SO)] <- "DEERE"
dataset$AU[grepl("KUBOTA", dataset$SO)] <- "KUBOTA"
table(dataset$AU) %>% sort(decreasing = TRUE)

#########################
# Retain the target of the study
dataset_original <- dataset
dataset <- dataset %>% filter(AU != 'OTHER')


#################################################
# Save and FINISH!
#################################################
## Create directories
save(dataset, 
     #dataset_original,
     dataset_metadata, 
     file = file.path(bibliometrics_folder, 
                      dataset_metadata$query_id, 
                      "dataset.rdata"))
## 
write.csv(dataset, 
          row.names = FALSE,
          file.path(bibliometrics_folder, 
                    dataset_metadata$query_id, 
                    "dataset.csv"))
rm(list = ls())
