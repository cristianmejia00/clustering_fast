# ==================================================
# Created in R; 2016-03-14; Kajikawa-lab; C.Mejia
# Working fine as of 2018-11-26.
# ==================================================

# Call necessary libraries
library(plyr)
library(data.table)
library(dplyr)
library(stringr)

###########################################################################################
# OPTIONS
###########################################################################################
## Query_id
## This has de form Qxxx whith the query number from the query control file
dataset_metadata <- list(
  "query_id" = "Q311_innovativeness",
  "fukan_url" = "Not apply. Directly from WOS"
)

download_folder_name <- "Q311_innovativeness"

###########################################################################################
# Find system and root
if (Sys.info()["sysname"] == "Windows") {
  print("You are running R on a Windows machine")
  raw_data_folder_path <- "C:\\Users\\crist\\OneDrive\\Documentos"
  bibliometrics_folder_path <- ""
} else if (Sys.info()["sysname"] == "Darwin") {
  print("You are running R on a Mac machine")
  raw_data_folder_path <- "/Users/cristian/Library/CloudStorage/OneDrive-Personal/Documentos"
  bibliometrics_folder_path <- "/Users/cristian/Library/CloudStorage/GoogleDrive-cristianmejia00@gmail.com/My Drive"
} else {
  print("You are running R on a different operating system")
}

# Open a window to select the directory with the files to merge
myfolder <- file.path(raw_data_folder_path, "imacros", "downloads", download_folder_name)
paths_to_files <- list.files(path = myfolder, full.names = TRUE, pattern = "*.txt", recursive = TRUE)

###########################################################################################
## Path to the Bibliometrics folder

# From the pov of this very code, this is actually the output folder. Where the files generated by this code will be placed.
bibliometrics_folder_path <- file.path(bibliometrics_folder_path, "Bibliometrics_Drive")
dir.create(file.path(bibliometrics_folder_path, dataset_metadata$query_id), showWarnings = FALSE)


# Read each file and store them in a vector
# fread sometimes fails when reading the header, what to do?
list_of_all_files <- lapply(paths_to_files, function(a_path) {
  data1 <- fread(a_path, sep = "\t", stringsAsFactors = FALSE, check.names = FALSE, encoding = "UTF-8", quote = "")
  # data1 <- read_from_wos(a_path) # NOTE: DO NOT USE read_from_wos() from package OPNER5, it cut off the lines before finish it, hence it does not read PY and UT for all rows.
  # data1 <- read.table(a_path, sep = '\t', fill = TRUE, stringsAsFactors = FALSE, header = TRUE, check.names = FALSE, quote = "", comment.char="", encoding = "UTF-16")
  # data1 <- read.csv(a_path, stringsAsFactors = FALSE, check.names = FALSE)
  # data1 <- read.delim(a_path, stringsAsFactors = FALSE, check.names = FALSE, encoding = "UTF-16", sep = "\t")
  return(data1)
})

# Verify than the files have the expected number of rows: 500. Except for a few that were the tails.
plot(unlist(sapply(list_of_all_files, nrow))) # The number of rows in each file, mostly 500.

# Create the merged dataset
dataset <- rbind.fill(list_of_all_files)
dataset <- as.data.frame(dataset)
if (colnames(dataset)[1] == "V1") {
  colnames(dataset) <- c("PT", colnames(dataset)[3:length(colnames(dataset))], "END")
  dataset["END"] <- NULL
}

# check for possible errors
# Verify correct reading by inspecting the publication year.
# If several non numeric values are present, it means there, there was a problem reading the files.
names(dataset)[1:20]

# Remove duplicated records
dataset <- dataset[!duplicated(dataset$UT), ]

# A record without PY, EA, or CY can be NA or "" empty string. We normalize anything to NA
dataset$PY[dataset$PY == ""] <- NA
dataset$EA[dataset$EA == ""] <- NA
dataset$CY[dataset$CY == ""] <- NA

# Correct records without PY
table(dataset$PY)
table(is.na(dataset$PY))

dataset$PY[is.na(dataset$PY)] <- 2024

test <- sapply(dataset$PY, function(x) {
  return(nchar(as.character(x)))
}) %>% unname()

dataset <- dataset %>% filter(test == 4)

# Correct records without PY
table(dataset$PY)
table(is.na(dataset$PY))
dataset$TI[is.na(dataset$PY)]

dataset$PY <- as.character(dataset$PY)
for (i in c(1:ncol(dataset))) {
  dataset[, i] <- as.character(dataset[, i]) %>% enc2utf8()
}

# Save the file
write.csv(dataset,
  file = file.path(
    bibliometrics_folder_path,
    dataset_metadata$query_id,
    paste("dataset_", dataset_metadata$query_id, ".csv", sep = "")
  ),
  row.names = FALSE
)
rm(list = ls())

############################################################################
# Run this only if there are many missing PY
#
# # Conference papers may have the year in CY
# # Early access paper may have the year in EA
# # So, we are going to complete from there
# idx <- is.na(dataset$PY) & !is.na(dataset$CY)
# dataset$PY[idx] <- sapply(dataset$CY[idx], function(x) {substr(x,nchar(x)-4, nchar(x))}) %>% as.numeric()
#
# idx <- is.na(dataset$PY) & !is.na(dataset$EA)
# dataset$PY[idx] <- sapply(dataset$EA[idx], function(x) {substr(x,nchar(x)-4, nchar(x))}) %>% as.numeric()
#
# table(is.na(dataset$PY))
# # A way to infer the PY of an article is by looking at the cited references. An article will cite articles
# # that were published on the same year of publication at most. i.e. by extracting the max year in the CR
# # We know that the paper was published in that or a posterior year. Hence, we infer here that it was published
# # On the same year of the last reference.
#
# ## An extra way to know the PY is by looking at the copyright year in the Abstract. (But I have not yet implemented this)
# idx <- is.na(dataset$PY) & dataset$CR != ""
# CR_years_available <- str_extract_all(dataset$CR[idx], "[[:digit:]]{4},")
# CR_max_year <- sapply(CR_years_available, function(x) {
#   tmp <- gsub(",", "", x)
#   tmp <- as.numeric(tmp)
#   if (length(tmp) > 0) {
#     tmp <- tmp[tmp > 0]
#     tmp <- tmp[tmp <= 2021]
#     return(max(tmp))
#   } else {
#     return (NA)
#   }
# })
# dataset$PY[idx] <- CR_max_year
#
# table(is.na(dataset$PY))

# Check NA UTs
# test <- dataset[!grepl("^WOS", dataset$UT),]

#
# # Correct records without UT
# table(grepl("^WOS", dataset$UT))
#
# idx <- !grepl("^WOS", dataset$UT)
# #dataset$UT[idx] <- paste(as.character(dataset$PY[idx]), gsub("[[:punct:]]| ", "", tolower(dataset$TI[idx])), sep = "") %>% sapply(., function(x) {substr(x,1,50)})
# dataset$UT
# # Remove files without UT
# #dataset <- dataset[!nchar(dataset$UT) != 19,]


#
# # Remove columns that are not used anywhere (i.e. let only those that are used or can be used)
# # Usable columns as of 20181201
# usable_columns <- c("PT", "AU", "TI", "SO", "LA", "DT", "DE", "ID", "AB", "C1", "OI", "AF",
#                     "RP", "FU", "FX", "CR", "NR", "TC", "Z9", "U1", "U2", "PU", "SN", "J9",
#                     "JI", "PY", "VL", "IS", "BP", "EP", "AR", "DI", "PG", "WC", "SC","UT")
